{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Dom Ruan Suzano\n",
    "\n",
    "Nome: Hudson Monteiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualiza√ß√£o\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando algumas bibliotecas para limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dom\n",
      "[nltk_data]     Ruan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\dom ruan\\anaconda3\\lib\\site-packages (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "!pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirma√ß√£o do diret√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "C:\\Users\\Dom Ruan\\2¬∞ Semestre Eng\\C.Dados\\Projeto_1_CDados\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lise do Banco de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pepsi.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>namoral n tem refrigerante melhor q o da pepsi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a pepsi fez o logo baseado na gravidade sobre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@erongranth neo qu√≠mica, kalunga e pepsi acho ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mataria por um refri geladinho agr mas so tem ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pitta_ai e dion√≠sio, o cara viciado em pepsi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@jujuubuenoo pepsi¬Æ geladinha tem um gostinho ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>escreva isso ou aquilo usando a tag #sonkezsen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@elitavres pepsi¬Æ √© tudo de bom, fala a√≠! üòâüíô‚ù§</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@pepsibr pq as pepsi ta vindo tao sem gas????</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ele s√≥ queria uma pepsi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relev√¢ncia\n",
       "0  namoral n tem refrigerante melhor q o da pepsi...           1\n",
       "1  a pepsi fez o logo baseado na gravidade sobre ...           1\n",
       "2  @erongranth neo qu√≠mica, kalunga e pepsi acho ...           1\n",
       "3  mataria por um refri geladinho agr mas so tem ...           1\n",
       "4      @pitta_ai e dion√≠sio, o cara viciado em pepsi           1\n",
       "5  @jujuubuenoo pepsi¬Æ geladinha tem um gostinho ...           0\n",
       "6  escreva isso ou aquilo usando a tag #sonkezsen...           0\n",
       "7      @elitavres pepsi¬Æ √© tudo de bom, fala a√≠! üòâüíô‚ù§           0\n",
       "8      @pepsibr pq as pepsi ta vindo tao sem gas????           1\n",
       "9                            ele s√≥ queria uma pepsi           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@trishteza uma pepsi¬Æ √© coisa de outro mundo, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@cultwolverine pepsi arena ia ficar muito mais...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@alisonbap pepsi e melhor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realizando uma pesquisa cient√≠fica com meus am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pizza tomando minha pepsi com lim√£o melhorou o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  Relev√¢ncia\n",
       "0  @trishteza uma pepsi¬Æ √© coisa de outro mundo, ...           0\n",
       "1  @cultwolverine pepsi arena ia ficar muito mais...           1\n",
       "2                          @alisonbap pepsi e melhor           1\n",
       "3  realizando uma pesquisa cient√≠fica com meus am...           0\n",
       "4  pizza tomando minha pepsi com lim√£o melhorou o...           1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nosso produto √© o refrigerante pepsi,um refrigerante norte-americano com sabor de cola, consideramos como relevante os tweets que falavam bem ou mal tanto da marca como do produto, al√©m disso exce√ß√µes  como tweets feito pelo twitter da pepsi foram considerados irrelevantes mesmo que falassem bem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo fun√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(lista):\n",
    "    nova_lista = []\n",
    "    for a in lista:\n",
    "        if a not in stopwords.words('portuguese'):\n",
    "            nova_lista.append(a)\n",
    "    return nova_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa os tweets, tira os pontos desnecess√°rios\n",
    "import re \n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punctuation = '[¬Æ,!-.:?;]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_emoji(text):\n",
    "    modified=' '.join(emoji.get_emoji_regexp().split(text))\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retira o arroba(@)\n",
    "def limpa_arrobas(texto):\n",
    "    novo_texto = []\n",
    "    for t in texto.split():\n",
    "        if t[0] != '@':\n",
    "            novo_texto.append(t)\n",
    "    novo_texto = ' '.join(novo_texto)\n",
    "    return novo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa os links do tweet\n",
    "def limpa_links(texto):\n",
    "    novo_texto = []\n",
    "    for t in texto.split():\n",
    "        if len(t) > 4 and t[0:4] != 'http':\n",
    "            novo_texto.append(t)\n",
    "    novo_texto = ' '.join(novo_texto)\n",
    "    return novo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fun√ß√£o que deixa todas as letras de uma frase minusc√∫las e separa ela em uma lista\n",
    "def tokenization(string):\n",
    "    return string.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(texto):\n",
    "    texto = limpa_arrobas(limpa_links(separa_emoji(cleanup(texto))))\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando num texto\n",
    "for a in range(300):\n",
    "    texto = train.loc[a,\"Treinamento\"]\n",
    "    texto = str(texto)\n",
    "    texto = limpa_texto(texto)\n",
    "    train.loc[a,\"Treinamento\"] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(200):\n",
    "    texto = test.loc[a,\"Teste\"]\n",
    "    texto = str(texto)\n",
    "    texto = limpa_texto(texto)\n",
    "    test.loc[a,\"Teste\"] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar palavras em vari√°veis categ√≥ricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando as fun√ß√µes de limpeza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando os tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevantes\n",
    "\n",
    "frequ√™ncias Absolutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treinamento                                                                        Relev√¢ncia\n",
       "pepsi                                                                              1             9\n",
       "pepsi melhor                                                                       1             4\n",
       "√∫nica coisa compensou pepsi infelizmente daquela garrafinha                        1             1\n",
       "gosto pepsi pepsi twist outro n√≠vel otimo refrigerante                             1             1\n",
       "gostar pepsi negar d√©cadas fazem campanhas marketing ser√£o lembradas sempre delas  1             1\n",
       "                                                                                                ..\n",
       "pepsi melhor passagem                                                              1             1\n",
       "pepsi melhor dependente tratar                                                     1             1\n",
       "pepsi maracuj√°                                                                     1             1\n",
       "pepsi maior melhor                                                                 1             1\n",
       "acabei fazer descoberta pepsi black a√ß√∫car melhor                                  1             1\n",
       "Length: 148, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevantes \n",
    "rel = train[train['Relev√¢ncia'] ==1]\n",
    "\n",
    "# frequencia absoluta\n",
    "rel_freq_abs = rel.value_counts()\n",
    "rel_freq_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrelevantes\n",
    "\n",
    "frequ√™ncias Absolutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treinamento                                        Relev√¢ncia\n",
       "pepsi                                              0             7\n",
       "triangular bread                                   0             3\n",
       "minha buceta gosto pepsi                           0             2\n",
       "√©poca √≥culos festa pepsi achando arrasando         0             1\n",
       "ent√£o psic√≥logo pepsi pessoas pensam sinceramente  0             1\n",
       "                                                                ..\n",
       "pepsi gtgtgtgtgt respeito tijucano                 0             1\n",
       "pepsi kkkkkk                                       0             1\n",
       "pepsi nossa reles compreens√£o                      0             1\n",
       "pepsi pegar largar                                 0             1\n",
       "                                                   0             1\n",
       "Length: 132, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Irrelevantes\n",
    "irr = train[train['Relev√¢ncia'] ==0]\n",
    "\n",
    "# Frequancia absoluta\n",
    "irr_freq_abs = irr.value_counts()\n",
    "\n",
    "irr_freq_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Relevantes\n",
    "palavras_relevantes = rel['Treinamento']\n",
    "relev_string = ''\n",
    "for tweet in palavras_relevantes:\n",
    "    relev_string += ' ' + tweet\n",
    "palav_relev = stop_words(tokenization(relev_string))\n",
    "\n",
    "# Irrelevantes\n",
    "palavras_irrelevantes = irr['Treinamento']\n",
    "irrelev_string = ''\n",
    "for tweet in palavras_irrelevantes:\n",
    "    irrelev_string += ' ' + tweet\n",
    "palav_irrelev = stop_words(tokenization(irrelev_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd Series de todas as palavras\n",
    "todas_palavras = palav_relev + palav_irrelev\n",
    "serie_todas_palavras = pd.Series(todas_palavras)\n",
    "\n",
    "# frequencia todas palavras relativa\n",
    "tabela_todas_palavras_relativa = serie_todas_palavras.value_counts(True)\n",
    "tabela_todas_palavras_relativa\n",
    "\n",
    "#frequencia de Todas palavras absolutas\n",
    "tabela_todas_palavras_abs = serie_todas_palavras.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras sem repeti√ß√£o\n",
    "palavras_sem_repeticao = list(set(todas_palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabela relevantes e irrelevantes\n",
    "serie_relevantes = pd.Series(palav_relev)\n",
    "serie_irrelevantes = pd.Series(palav_irrelev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencias relativas relevantes e irrelevantes\n",
    "tabela_relevantes_relativa = serie_relevantes.value_counts(True)\n",
    "tabela_irrelevantes_relativa = serie_irrelevantes.value_counts(True)\n",
    "\n",
    "rel_freq_abs = serie_relevantes.value_counts()\n",
    "irr_freq_abs = serie_irrelevantes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " # texto explicando as probabilides que queremos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilides iniciais\n",
    "\n",
    "# calculando a probabilidade de ser relevante e irrelente\n",
    "P_rel = len(palav_relev)/len(todas_palavras)\n",
    "P_irr = len(palav_irrelev)/len(todas_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo ‚ÄúNaive Bayes‚Äù √© um classificador probabil√≠stico baseado no ‚ÄúTeorema de Bayes‚Äù, o qual foi criado por Thomas Bayes (1701 - 1761) para tentar provar a exist√™ncia de Deus.\n",
    "\n",
    "Atualmente, esse algoritmo √© utilizado na √°rea de $Data Science$ tendo a funcionalidade de categorizar textos com base na frequ√™ncia de palavras que aparecem no mesmo.\n",
    "\n",
    "A melhor parte deste algoritmo vem da sua ingenuidade ($naive$ em ingl√™s)! Com isso n√≥s queremos dizer que ele desconsidera a correla√ß√£o de palavras(vari√°veis) e trata elas de forma independente.\n",
    "\n",
    "Mas agora vamos as contas! Como de fato n√≥s podemos utilizar esse algoritmo na programa√ß√£o? Bom, trazendo esse projeto como exemplo, temos:\n",
    "\n",
    "O teorema de Naive Bayes consiste em calcular\n",
    "\n",
    "probabilidades partindo de eventos posteriores (ser relevante, dado que foi classificado como relevante), multiplicando pela probabilidade de o tweet ser de fato relevante pela probabilidade ‚Äúser classificado como relevante, dado que o tweet √© relevante‚Äù. De forma an√°loga a isso se aplica para os tweets irrelevantes.\n",
    "\n",
    "Como a soma dessas probabilidades para os tweets relevantes e irrelevantes deve resultar em 1, √© necess√°rio normalizar esses valores e isso √© feito dividindo cada um desses pela soma de ambos.\n",
    "\n",
    "Mas agora vamos mostrar isso com um pouco de matem√°tica:\n",
    "\n",
    "Definindo algumas probabilidades:\n",
    "\n",
    "\n",
    "$P(Tweet|Relevante)$: probabilidade de classificar o tweet como relevante dado que o tweet √© relevante;\n",
    "$P(Tweet|Irrelevante)$: probabilidade de classificar o tweet como irrelevante dado que o twwet √© irrelevante;\n",
    "$P(Relevante)$: probabilidade do tweet ser relevante;\n",
    "$P(Irrelevante)$: probabilidade do tweet ser irrelevante;\n",
    "$P(Tweet)$: probabilidade de um tweet qualquer ocorrer.\n",
    "$$P(Relevante|Tweet) = \\frac{P(Tweet|Relevente) P(Relevante)}{P(Tweet)}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "O processo √© an√°logo para $P(Irrelevante|tweet)$:\n",
    "\n",
    "\n",
    "\n",
    "$$P(Irrelevante|Tweet) = \\frac{P(Tweet|Irrelevante) P(Irrelevante)}{P(Tweet)}$$\n",
    "\n",
    "\n",
    "Mas como podemos calcular cada um desses termos para saber se o tweet √© relevante ou irrelevante?\n",
    "Bom, primeiramente vamos calcular as seguintes probabilidades: $P(Relevante)$ e $P(Irrelevante)$.\n",
    "\n",
    "Quantidade de palavras dos tweets relevantes: $Qr$\n",
    "Quantidadede de palavras dos tweets irrelevantes: $Qirr$\n",
    "Quantidade de palavras totais: $Qt$\n",
    "\n",
    "\n",
    "$$P(Relevante) = \\frac{Qr}{Qt}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$P(Irrelevante) = \\frac{Qirr}{Qt}$$\n",
    "\n",
    "\n",
    "Feito isso, vamos calcular as seguinte probabilidades: $P(Tweet|Relevante)$ e $P(Tweet|Irrelevante)$.\n",
    "\n",
    "Para calcular essas probabilidades √© necess√°rio separar as\n",
    "\n",
    "palavras de um determinado tweet, observar a frequ√™ncia absoluta de vezes que essa palavra aparece no total de palavras relevantes ou irrelevantes, e dividir pelo valor total de palavras que est√£o no conjunto de palavras relevantes ou irrelevantes. Ap√≥s feito isso √© necess√°rio multiplicar o valor obtido para cada palavra e assim temos a probabilidade $P(Tweet|Relevante ou Irrelevante)$ para um determinado tweet. Isso tudo gra√ßas a independ√™ncia de palavras do algoritmo Naive Bayes. Mas esse m√©todo tem um problema, e se a frequ√™ncia absoluta de uma determinada palavra for 0, ou seja, se ela n√£o estiver na nossa base de dados treinamento ?\n",
    "\n",
    "\n",
    "\n",
    "O Classificador de Naive Bayes se baseia na constru√ß√£o de um modelo bag-of-word. Na an√°lise de sentimento, queremos responder a seguinte pergunta: \"Qual a probabilidade dessa frase ser relevante, dado esse conjunto de palavras?\". Nesse sentido, √© necess√°rio computar esse c√°lculo utilizando probabilidades condicionais e o resultado do teorema de Bayes encontrado na sess√£o anterior:\n",
    "\n",
    "\n",
    "Dessa forma, n√≥s vamos usar essa rela√ß√£o para encontrar $P(R|frase)$, ou seja, a probabilidade de uma frase ser relevante, dado o conjunto de palavras. Se quisermos encontrar essa probabilidade faremos a seguinte opera√ß√£o:\n",
    "$$P(R|frase) = \\frac{P(frase|R).P(R)}{P(frase)}$$\n",
    "Para prosseguir, utilizaremos um processo de \"Tokeniza√ß√£o\", que consiste em dividir a frase em peda√ßos menores (as palavras) e assumir que uma palavra n√£o influencia na coloca√ß√£o da outra. Sabemos que isso n√£o √© verdade, mas utilizaremos por quest√µes de simplifica√ß√£o (essa √© a ingenuidade do classificador de Naive Bayes)\n",
    "\n",
    "De forma an√°loga √© possivel encontrar:\n",
    "$$P(I|frase) = \\frac{P(frase|I).P(I)}{P(frase)}$$\n",
    "\n",
    "Agora, basta compararmos os valores das probabilidades:\n",
    "\n",
    "Se, $P(R|frase) > P(I|frase)$, ent√£o, √© mais prov√°vel que a frase seja $relevante$\n",
    "\n",
    "Caso contr√°rio, $P(R|frase) < P(I|frase)$, ent√£o, √© mais prov√°vel que a frase seja $irrelevante$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suaviza√ß√£o de Laplace\n",
    "\n",
    "Para corrigir este problema, pode ser usado a corre√ß√£o Laplaciana, que se resume na adi√ß√£o de registros ‚Äúfict√≠cios‚Äù no conjunto de treinamento, de forma que n√£o apare√ßam atributos de frequ√™ncia zero e o problema seja contornado. Vale destacar, que a adi√ß√£o de registros ir√° causar a mudan√ßa dos valores de probabilidade calculados inicialmente.\n",
    "Se a quantidade de dados utilizados para treinar o modelo for pequena, a aplica√ß√£o dessa t√©cnica surtir√° numa maior distor√ß√£o sobre os dados originais.\n",
    "\n",
    "$$P(Palavra|Relevante) = \\frac{0 + 1}{Qr + Qtp}$$\n",
    "\n",
    "Sendo assim, n√≥s teremos uma f√≥rmula geral descrita da forma:\n",
    "\n",
    "Frequ√™ncia absoluta da palavra na lista de palavras relevantes: $Far$\n",
    "Frequ√™ncia absoluta da palavra na lista de palavras irrelevantes: $Fairr$\n",
    "\n",
    "\n",
    "$$P(Palavra|Relevante) = \\frac{Far + 1}{Qr + Qtp}$$\n",
    "\n",
    "\n",
    "O racioc√≠nio √© an√°logo para a lista irrelevante:\n",
    "\n",
    "\n",
    "\n",
    "$$P(Palavra|Irrelevante) = \\frac{Fairr + 1}{Qirr + Qtp}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suaviza√ß√£o de Laplace para corrigir possiveis palavras fora da minha base de dados\n",
    "def smoothing(palavra, frequencia_absoluta, palavras_sem_repeticao, relevancia, alpha=1):\n",
    "    resultado = 0\n",
    "    try:\n",
    "        absoluta = frequencia_absoluta[palavra]\n",
    "    except:\n",
    "        absoluta = 0 #Se a palavra n√£o estiver na lista retorna 0\n",
    "    if relevancia == 'relevante':\n",
    "        resultado = (absoluta + alpha)/(len(palav_relev)+alpha*len(palavras_sem_repeticao))\n",
    "    elif relevancia == 'irrelevante':\n",
    "        resultado = (absoluta + alpha)/(len(palav_irrelev)+alpha*len(palavras_sem_repeticao))\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√°lculo das Probabilidades condicionais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidade para fazer a desigualdade \n",
    "def prob(relevancia,frase):\n",
    "    if relevancia == 'relevante':\n",
    "        probTweetdadoR = 1\n",
    "        for a in frase.split():\n",
    "            probTweetdadoR *= smoothing(a, rel_freq_abs, palavras_sem_repeticao, relevancia)\n",
    "        probRdadoTweet = P_rel * probTweetdadoR\n",
    "        return probRdadoTweet\n",
    "    elif relevancia == 'irrelevante':\n",
    "        probTweetdadoI = 1\n",
    "        for b in frase.split():\n",
    "            probTweetdadoI *= smoothing(b, irr_freq_abs, palavras_sem_repeticao, relevancia)\n",
    "        probIdadoTweet = P_irr * probTweetdadoI\n",
    "        return probIdadoTweet\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador de Naive Bayes\n",
    "\n",
    "O Classificador de Naive Bayes se baseia na constru√ß√£o de um modelo bag-of-word. Na an√°lise de sentimento, queremos responder a seguinte pergunta: \"Qual a probabilidade dessa frase ser relevante, dado esse conjunto de palavras?\". Nesse sentido, √© necess√°rio computar esse c√°lculo utilizando probabilidades condicionais e o resultado do teorema de Bayes encontrado na sess√£o anterior:\n",
    "\n",
    "\n",
    "Dessa forma, n√≥s vamos usar essa rela√ß√£o para encontrar $P(R|frase)$, ou seja, a probabilidade de uma frase ser relevante, dado o conjunto de palavras. Se quisermos encontrar essa probabilidade faremos a seguinte opera√ß√£o:\n",
    "$$P(R|frase) = \\frac{P(frase|R).P(R)}{P(frase)}$$\n",
    "Para prosseguir, utilizaremos um processo de \"Tokeniza√ß√£o\", que consiste em dividir a frase em peda√ßos menores (as palavras) e assumir que uma palavra n√£o influencia na coloca√ß√£o da outra. Sabemos que isso n√£o √© verdade, mas utilizaremos por quest√µes de simplifica√ß√£o (essa √© a ingenuidade do classificador de Naive Bayes)\n",
    "\n",
    "De forma an√°loga √© possivel encontrar:\n",
    "$$P(I|frase) = \\frac{P(frase|I).P(I)}{P(frase)}$$\n",
    "\n",
    "Agora, basta compararmos os valores das probabilidades:\n",
    "\n",
    "Se, $P(R|frase) > P(I|frase)$, ent√£o, √© mais prov√°vel que a frase seja $relevante$\n",
    "\n",
    "Caso contr√°rio, $P(R|frase) < P(I|frase)$, ent√£o, √© mais prov√°vel que a frase seja $irrelevante$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de naive para comparar as probabilidades\n",
    "def Naive(tweet):\n",
    "    if prob('relevante',tweet) > prob('irrelevante',tweet):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Classificador\"]=train[\"Treinamento\"].apply(Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "      <th>Classificador</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>namoral refrigerante melhor pepsip existe nenhum</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pepsi baseado gravidade sobre pepsi sentido ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qu√≠mica kalunga pepsi patrocinadores ficaram b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mataria refri geladinho pepsi geladeira</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dion√≠sio viciado pepsi</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>queria fazer drink laranja beber pepsi mesmo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>minha falando pepsi caber geladeira ‚Äúgordinha‚Äù...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>triangular bread</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>acordei enjoo kerelho minha desculpa tomar cop...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>pessoas pepsi kkkkkk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Treinamento  Relev√¢ncia  \\\n",
       "0     namoral refrigerante melhor pepsip existe nenhum           1   \n",
       "1    pepsi baseado gravidade sobre pepsi sentido ne...           1   \n",
       "2    qu√≠mica kalunga pepsi patrocinadores ficaram b...           1   \n",
       "3              mataria refri geladinho pepsi geladeira           1   \n",
       "4                               dion√≠sio viciado pepsi           1   \n",
       "..                                                 ...         ...   \n",
       "295       queria fazer drink laranja beber pepsi mesmo           0   \n",
       "296  minha falando pepsi caber geladeira ‚Äúgordinha‚Äù...           0   \n",
       "297                                   triangular bread           0   \n",
       "298  acordei enjoo kerelho minha desculpa tomar cop...           1   \n",
       "299                               pessoas pepsi kkkkkk           0   \n",
       "\n",
       "     Classificador  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "..             ...  \n",
       "295              0  \n",
       "296              0  \n",
       "297              0  \n",
       "298              1  \n",
       "299              0  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A porcentagem de verdadeiros positivos foi de 52.67%\n",
      "A porcentagem de falsos positivos foi de 4.00%\n"
     ]
    }
   ],
   "source": [
    "verdadeiros_positivos = train.loc[(train.Relev√¢ncia == 1) & (train.Classificador == 1), :].shape[0]\n",
    "falsos_positivos = train.loc[(train.Relev√¢ncia == 0) & (train.Classificador == 1), :].shape[0]\n",
    "\n",
    "print(f'A porcentagem de verdadeiros positivos foi de {100*verdadeiros_positivos/train.shape[0]:.2f}%')\n",
    "print(f'A porcentagem de falsos positivos foi de {100*falsos_positivos/train.shape[0]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A porcentagem de verdadeiros positivos foi de 43.00%\n",
      "A porcentagem de falsos positivos foi de 0.33%\n"
     ]
    }
   ],
   "source": [
    "verdadeiros_negativos = train.loc[(train.Relev√¢ncia == 0) & (train.Classificador == 0), :].shape[0]\n",
    "falsos_negativos = train.loc[(train.Relev√¢ncia == 1) & (train.Classificador == 0), :].shape[0]\n",
    "\n",
    "print(f'A porcentagem de verdadeiros positivos foi de {100*verdadeiros_negativos/train.shape[0]:.2f}%')\n",
    "print(f'A porcentagem de falsos positivos foi de {100*falsos_negativos/train.shape[0]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.67%\n"
     ]
    }
   ],
   "source": [
    "acuracia = 100*(verdadeiros_positivos+verdadeiros_negativos)/train.shape[0]\n",
    "print(f'{acuracia:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "      <th>Classificador</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pepsi coisa outro mundo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pepsi arena ficar muito empresa droga controla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pepsi melhor</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realizando pesquisa cient√≠fica amigos saber co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pizza tomando minha pepsi lim√£o melhorou humor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>querer chato pessoal parece nunca refrigerante...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>sonha pepsi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>dropei cocacola beber pepsi homenagem minha qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>esmagamento pepsi padr√£o a√ß√∫car sendo dif√≠cil ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>pedindo pepsi geladinha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Relev√¢ncia  \\\n",
       "0                              pepsi coisa outro mundo           0   \n",
       "1    pepsi arena ficar muito empresa droga controla...           1   \n",
       "2                                         pepsi melhor           1   \n",
       "3    realizando pesquisa cient√≠fica amigos saber co...           0   \n",
       "4    pizza tomando minha pepsi lim√£o melhorou humor...           1   \n",
       "..                                                 ...         ...   \n",
       "195  querer chato pessoal parece nunca refrigerante...           1   \n",
       "196                                        sonha pepsi           0   \n",
       "197  dropei cocacola beber pepsi homenagem minha qu...           0   \n",
       "198  esmagamento pepsi padr√£o a√ß√∫car sendo dif√≠cil ...           1   \n",
       "199                            pedindo pepsi geladinha           0   \n",
       "\n",
       "     Classificador  \n",
       "0                1  \n",
       "1                0  \n",
       "2                1  \n",
       "3                0  \n",
       "4                1  \n",
       "..             ...  \n",
       "195              1  \n",
       "196              1  \n",
       "197              0  \n",
       "198              1  \n",
       "199              0  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"Classificador\"] = test[\"Teste\"].apply(Naive)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A porcentagem de verdadeiros positivos foi de 38.50%\n",
      "A porcentagem de falsos positivos foi de 28.00%\n"
     ]
    }
   ],
   "source": [
    "verdadeiros_positivos = test.loc[(test.Relev√¢ncia == 1) & (test.Classificador == 1), :].shape[0]\n",
    "falsos_positivos = test.loc[(test.Relev√¢ncia == 0) & (test.Classificador == 1), :].shape[0]\n",
    "\n",
    "print(f'A porcentagem de verdadeiros positivos foi de {100*verdadeiros_positivos/test.shape[0]:.2f}%')\n",
    "print(f'A porcentagem de falsos positivos foi de {100*falsos_positivos/test.shape[0]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A porcentagem de verdadeiros negativos foi de 19.00%\n",
      "A porcentagem de falsos negativos foi de 14.50%\n"
     ]
    }
   ],
   "source": [
    "verdadeiros_negativos = test.loc[(test.Relev√¢ncia == 0) & (test.Classificador == 0), :].shape[0]\n",
    "falsos_negativos = test.loc[(test.Relev√¢ncia == 1) & (test.Classificador == 0), :].shape[0]\n",
    "\n",
    "print(f'A porcentagem de verdadeiros negativos foi de {100*verdadeiros_negativos/test.shape[0]:.2f}%')\n",
    "print(f'A porcentagem de falsos negativos foi de {100*falsos_negativos/test.shape[0]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.50%\n"
     ]
    }
   ],
   "source": [
    "acuracia = 100*(verdadeiros_positivos+verdadeiros_negativos)/test.shape[0]\n",
    "print(f'{acuracia:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No classificador que tem como objetivo de encontrar a relev√¢ncia dos tweets relacionas a marca Pepsi, como visto acima foi obtido uma acur√°cia de 57.50%, com o percentual de 38.50% verdadeiros positivos e 19.00% negativos negativos, foi obtido tamb√©m um percentual de 28.00% de falsos positivos e 14.50% de falsos negativos. \n",
    "\n",
    "\n",
    "Por que n√£o podemos usar o pr√≥prio classificador para gerar mais amostras de treinamento?\n",
    "\n",
    "Nosso algoritmo de Naive Bayes a planilha de treinamento como algo que seja ideal para treinar o algoritmo,com isso tenta classificar outros tweets com base no que ela \"aprendeu\" observando a mesma. Isso faz com que esse classificador tenha problemas de classifica√ß√£o dado que se tem uma base pequena de dados para garantir a classifica√ß√£o dos demais. Para que fosse poss√≠vel gerar mais amostras seria necess√°rio ir atualizando a tabela treinamento para que o classificador aumente sua base de dados e fique mais preparado para classificar novos tweets de forma mais eficiente.\n",
    "\n",
    "Diferentes cen√°rios para Na√Øve Bayes\n",
    "\n",
    "Uma das principais utilidades do algoritmo de $Naive Bayes$ no contexto contempor√¢neo √© na distin√ß√£o de e-mails para poder filtrar os indesejados $spams$ e para qualificar textos baseando-se na frequ√™ncia de palavras usadas.\n",
    "\n",
    "O algoritmo de Naive Bayes pode ser utilizado para diminuir os acidentes de tr√¢nsitos. Com base nos movimentos oculares de motoristas que dirigiram cansados e guardando esses resultados em uma base de dados, seria poss√≠vel alertar aos motoristas quando eles estivessem cansados para que fizessem um descan√ßo antes de seguir viagem. Isso diminuiria dr√°sticamente os acidentes em rodovias.\n",
    "\n",
    "Uma outra utilidade para essa ferramenta seria na distin√ß√£o de mensagens suspeitas em redes socias para evitar os golpes virtuais que est√£o crescendo com o avan√ßo da tecnologia. O algoritmo seria capaz de identificar as palavras mais frequentes nesse estilo de mensagens e sinalizar o usu√°rio de que aquela mensagem tem um potencial risco de ser um golpe.\n",
    "\n",
    "Naive Bayes poderia tamb√©m ser usado para direcionamento de pesquisas na internet. Guardando palavras-chave na sua base de dados e direcionando publicidades que encaixam com o perfil do usu√°rio baseado no seu hist√≥rico de pesquisa.\n",
    "\n",
    "\n",
    "Cr√≠ticas e futuras itera√ß√µes:\n",
    "\n",
    "Para melhorar nosso classificador seria interessante trein√°-lo com diferentes tweets na base de treinamento. Para isso seria necess√°rio mesclar as planilhas de treinamento e teste, depois separ√°-las de forma aleat√≥ria e repetir o processo. Fazendo isso repetidas vezes seria poss√≠vel observar uma melhora na acur√°cia do nosso classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geral</th>\n",
       "      <th>Relev√¢ncia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>namoral refrigerante melhor pepsip existe nenhum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pepsi baseado gravidade sobre pepsi sentido ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qu√≠mica kalunga pepsi patrocinadores ficaram b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mataria refri geladinho pepsi geladeira</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dion√≠sio viciado pepsi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>querer chato pessoal parece nunca refrigerante...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>sonha pepsi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>dropei cocacola beber pepsi homenagem minha qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>esmagamento pepsi padr√£o a√ß√∫car sendo dif√≠cil ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>pedindo pepsi geladinha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Geral  Relev√¢ncia\n",
       "0     namoral refrigerante melhor pepsip existe nenhum           1\n",
       "1    pepsi baseado gravidade sobre pepsi sentido ne...           1\n",
       "2    qu√≠mica kalunga pepsi patrocinadores ficaram b...           1\n",
       "3              mataria refri geladinho pepsi geladeira           1\n",
       "4                               dion√≠sio viciado pepsi           1\n",
       "..                                                 ...         ...\n",
       "195  querer chato pessoal parece nunca refrigerante...           1\n",
       "196                                        sonha pepsi           0\n",
       "197  dropei cocacola beber pepsi homenagem minha qu...           0\n",
       "198  esmagamento pepsi padr√£o a√ß√∫car sendo dif√≠cil ...           1\n",
       "199                            pedindo pepsi geladinha           0\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split = pd.read_excel(filename, sheet_name = 'Teste').rename(columns={'Teste':'Geral'})\n",
    "train_split = pd.read_excel(filename, sheet_name = 'Treinamento').rename(columns={'Treinamento':'Geral'})\n",
    "\n",
    "#Concatenando\n",
    "data_total = pd.concat([train_split,test_split])\n",
    "\n",
    "#Limpando os tweets\n",
    "data_total['Geral'] = data_total['Geral'].apply(limpa_texto)\n",
    "data_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca necess√°ria para fatiamento do conjunto de dados\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=[]\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    #training_data, testing_data = train_test_split(full_data, test_size=0.2, random_state=random.randint(1,500))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_total[['Geral','Relev√¢ncia']],data_total.Relev√¢ncia,\n",
    "    test_size=0.3)\n",
    "    \n",
    "    palavras_relevantes = X_train[X_train['Relev√¢ncia']==1]\n",
    "    relev_string = ''\n",
    "    for tweet in palavras_relevantes:\n",
    "        relev_string += ' ' + tweet\n",
    "    palav_relev = stop_words(tokenization(relev_string))\n",
    "    \n",
    "    palavras_irrelevantes = X_train[X_train['Relev√¢ncia']==0]\n",
    "    irrelev_string = ''\n",
    "    for tweet in palavras_irrelevantes:\n",
    "        irrelev_string += ' ' + tweet\n",
    "    palav_irrelev = stop_words(tokenization(irrelev_string))\n",
    "    \n",
    "    serie_relevantes = pd.Series(palav_relev)\n",
    "    serie_irrelevantes = pd.Series(palav_irrelev)\n",
    "    \n",
    "    # Gera duas listas com as palavras relevantes e irrelevantes, gera uma lista total e uma lista das palavras sem repeti√ß√£o\n",
    "    palavras_rel = list(palav_relev)\n",
    "    palavras_irrel = list(palav_irrelev)\n",
    "    palavras = palavras_rel + palavras_irrel\n",
    "    palavras_sem_repeticao = list(set(palavras))\n",
    "    \n",
    "    total = pd.Series(palavras)\n",
    "\n",
    "    # Frequencias absolutas\n",
    "    rel_freq_abs = serie_relevantes.value_counts()\n",
    "    irrel_freq_abs = serie_irrelevantes.value_counts()\n",
    "    tabela_todas_palavras_abs = total.value_counts()\n",
    "\n",
    "    P_R = len(palavras_rel) / len(total)\n",
    "\n",
    "    # Por complementar, temos P_Rc\n",
    "    P_Rc = len(palavras_irrel) / len(total)\n",
    "    \n",
    "    assert P_R+P_Rc==1\n",
    "\n",
    "    X_test['Classificador'] = X_test['Geral'].apply(Naive)\n",
    "\n",
    "    verdadeiros_positivos = X_test.loc[(X_test['Classificador'] == 1) & (X_test['Relev√¢ncia'] == 1),:].shape[0]\n",
    "    verdadeiros_negativos = X_test.loc[(X_test['Classificador'] == 0) & (X_test['Relev√¢ncia'] == 0),:].shape[0]\n",
    "    acuracia = (verdadeiros_positivos + verdadeiros_negativos)/X_test.shape[0]\n",
    "    lista.append(acuracia * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "faixa=np.arange(30,99,1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(lista, bins=faixa, edgecolor='black', density=False)\n",
    "plt.title('Frequencia dos scores do modelo')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.xlabel('Porcentagem de acerto (%)')\n",
    "plt.xlim(30,60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem m√°xima: 54.67%\n",
      "Porcentagem m√≠nima: 36.67%\n",
      "Porcentagem m√©dia: 46.21%\n"
     ]
    }
   ],
   "source": [
    "print(f'Porcentagem m√°xima: {max(lista):.2f}%')\n",
    "print(f'Porcentagem m√≠nima: {min(lista):.2f}%')\n",
    "print(f'Porcentagem m√©dia: {sum(lista)/len(lista):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
